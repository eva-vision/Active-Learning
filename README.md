# Active-Learning
## Methods: ğœ€-greedy, UCB1, UCB-ğ›¼:. Thompson, Bayesian UCB


Active Learning
Consider a 
ğ¾
K-armed bandit machine, where pulling a lever may reward us with some amount of reward. The rewards from each lever are random variables following unknown, independent distributions. Our goal is to pull the levers in sequence to:

Explore the problem space, i.e., estimate the "goodness" of each lever (exploration).
Exploit this knowledge to maximize our total rewards (exploitation).
We recognize that these two goals conflict: to explore, we must pull suboptimal levers, which reduces our total rewards. Due to the probabilistic nature of the levers, a "good" lever might occasionally give a bad reward, so it can be beneficial to test levers that seem bad more than once. This is the exploration vs. exploitation dilemma.

Although our strategies work for various distributions, we will use the well-known Bernoulli distribution. Each lever has an unknown parameter 
ğœƒ
ğ‘˜
Î¸ 
k
â€‹
 , representing the probability of receiving a reward of 1 unit from that lever. Thus, each pull corresponds to a Bernoulli trial, where 
ğ‘¦
âˆˆ
{
0
,
1
}
yâˆˆ{0,1} is the reward received after pulling the 
ğ‘˜
k-th lever. Let 
ğ‘¦
â€¾
ğ‘˜
y
â€‹
  
k
â€‹
  denote the average reward from the 
ğ‘˜
k-th lever, 
ğ‘›
ğ‘˜
n 
k
â€‹
  denote the number of times the 
ğ‘˜
k-th lever has been pulled, and 
ğ‘›
n denote the total number of pulls so far.
